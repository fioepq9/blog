---
title: 操作系统面试题
mathjax: true
date: 2022-04-06 16:50:51
tags:
  - 操作系统
categories:
  - 面试题
---

## 进程、线程、协程之间的区别？

1. 进程是一个运行中的程序实例，是CPU资源调度的基本单位；线程是运行在进程上下文的逻辑控制流，是程序执行的基本单位；协程是用户态的轻量级线程，是线程内部调度的基本单位。线程依赖于进程，一个进程里至少存在一个线程；协程依赖于线程，一个线程里至少存在一个协程。

2. 进程持有的资源有：进程ID、页表（提供了一个独立于其它进程的地址空间）、文件描述符表、代码段、数据段、环境变量、内核栈、用户栈、寄存器中的内容（通用寄存器、flag寄存器、程序计数器）；

   线程持有的资源有：线程ID、寄存器中的内容（通用寄存器、flag寄存器、程序计数器）、内核栈、用户栈；

   协程持有的资源有：寄存器中的内容（通用寄存器、flag寄存器、程序计数器）、用户栈。

3. 切换开销上：进程 &gt; 线程 &gt; 协程。切换，实质上就是将当前上下文保存，然后取出新的上下文，显然持有的资源越多，切换的开销也就越大；同时，进程切换的时候还会刷新快表，导致快表缓存失效，也就导致新进程刚启动的时候运行速率较慢。进程、线程切换时都要陷入内核态，而协程不需要。进程、线程的切换由操作系统调度，而协程由用户自己调度。
4. 通信方面上：进程间的通信主要依靠IPC；而线程、协程间的通信，因为同一进程下的线程、协程共享内存，通信可以直接通过内存进行。

### 同一进程中的线程可以共享哪些数据？

进程ID、页表、文件描述符表、代码段、数据段、环境变量。

### 线程独占哪些资源？

线程ID、寄存器中的内容、内核栈、用户栈。

## 进程是如何进行切换的？

进程的切换是由操作系统进行的。操作系统首先会触发一个系统调用，进入到内核态；在内核态中，将当前进程的上下文保存，然后通过进程调度算法从阻塞态的进程队列中取出一个进程，恢复新进程的上下文，然后退出内核态，将控制权移交给新进程。

### 进程的状态有哪些？

#### 五态模型

+ 新建态：进程刚刚被创建，等待系统完成创建进程的所有必要信息。
+ 就绪态：一个进程得到了除控制权以外的所需资源，一旦得到控制权就可以执行。
+ 运行态：一个进程在运行的过程中。
+ 阻塞态：一个进程正在等待某一事件的发生，或者说等待一个信号而暂时停止运行。
+ 终止态：进程已结束运行，回收除进程控制块（PCB）之后的其他资源，并等待其他进程从PCB中收集相关信息。

### 进程调度策略有哪些？

#### 批处理系统

1. 先来先服务(FCFS)

按照请求的顺序进行调度。这是一种非抢占式的算法，开销小，没有饥饿问题。可能会导致短作业的响应时间过长。

2. 最短作业优先(SJF)

按估计运行时间最短的顺序进行调度。也是一种非抢占式的算法，吞吐量高。但如果一直有短作业到来，会导致长作业的饥饿。

3. 最短剩余时间优先(SRTN)

按估计剩余运行时间的顺序进行调度，是最短作业优先的抢占式版本，吞吐量高。跟最短作业优先有一样的问题，可能导致长作业饥饿。

#### 交互式系统

1. 时间片轮转(Round Robin)

将所有就绪进程按FCFS的原则排成队列，用完时间片的进程回到队列尾部。这种算法很好地兼顾了响应时间和处理时间，但时间片的大小选择比较困难。如果时间片过小，进程切换频繁，开销太大；如果时间片太长，实时性就得不到保证。

2. 优先级调度算法

为所有进程分配一个优先级，按优先级进行调度。为了防止低优先级的进程得不到调度，可以随时间增加等待进程的优先级。

3. 多级反馈队列调度算法(Multilevel Feedback Queue)

维护多个就绪进程队列，优先级递减，时间片递增。只有优先级更高的队列为空时才会调度低优先级队列中的队列。所有进程一开始都位于最高优先级队列，每次执行完一个时间片，就移动到下个队列。为了避免长作业饥饿问题，可以每隔一段时间，就重新将进程放入最高优先级队列。

### 进程间通信的方式（IPC）？

1. 管道（匿名管道、有名管道）
2. 消息队列
3. 共享内存
4. 信号量
5. 信号
6. 套接字

#### 讲讲你对同步和互斥的理解？

+ 同步：多个控制流因为合作而使得控制流的执行有一定的先后顺序。
+ 互斥：同一时间只有一个控制流能够执行。

## 什么是死锁？

多个控制流之间互相持有所需资源，并等待其它控制流释放资源，导致的循环等待的情况。

### 死锁产生的必要条件？

+ 互斥：一个资源一次只能被一个控制流持有。
+ 占有并等待：一个控制流至少占有一个资源，并在等待另一个被其它控制流占有的资源。
+ 非抢占：已经分配给一个控制流的资源不能被强制性抢占，只能由控制流完成任务后自行释放。
+ 循环等待：若干控制流之间形成一种头尾相接的环形等待资源关系，该环路中的每个控制流等在等待上一个控制流所占有的资源。

### 死锁的处理方法？

#### 鸵鸟策略

直接忽略死锁。因为解决死锁的问题的代价很高，当发生死锁时不会对用户造成多大影响，或者发生死锁的概率很低时，可以采用鸵鸟策略。

#### 死锁预防

基本思想是破坏形成死锁的四个必要条件。

+ 破坏互斥条件：允许某些资源同时被多个控制流持有。这个做法的问题是有些资源本身不具有这种属性，实用性有限。
+ 破坏占有并等待条件：实现资源预先分配策略，即一个控制执行任务之前，必须一次性申请所需资源，否则不允许。这个做法的问题是很多时候我们无法在执行前得知一个控制流具体需要哪些资源。同时这个也会降低资源的利用率，减少了并发量。
+ 破坏非抢占条件：允许控制流强行抢占其它控制流占有的资源。抢占会带来很多额外的开销。
+ 破坏循环等待条件：对资源进行编号，所有控制流对资源的申请必须按序提出，只有占有了低级资源的控制流才能申请高级资源。这样避免了占有高级资源的控制流再去申请低级资源，破坏了循环环路。

#### 死锁避免

动态检查资源分配状态，确保系统处于安全状态，只有处于安全状态时才会进行资源的分配。安全状态指的是：存在某种对控制流的资源分配顺序，使得每一个控制流都能运行。

#### 死锁解除

当检测到死锁的时候，让某些控制流回滚到足以解除死锁的地步，控制流回滚时释放资源。这要求系统维护控制流的历史信息，并设置回滚点。

## 虚拟内存是什么？

虚拟内存是一种对真实物理内存的抽象，它将物理内存看成是磁盘的缓存，为每个进程提供了独立的地址空间，保护了每个进程的地址空间不被其他进程破坏。

将一个进程内的虚拟地址转化成对应的物理地址，叫做地址翻译。计算机的地址翻译是软硬件结合的，os负责为每个进程维护页表，硬件MMU通过快表、页表执行地址翻译。MMU如果从页表中发现该页不在内存中，就会触发缺页异常，根据页面置换算法从已缓存的页中选择一页进行置换（如果牺牲页被修改过，就复制其回磁盘）。

### 虚拟内存的优点

+ 每个进程拥有独立的地址空间，这简化了内存管理，让进程间互不影响（通过在页表中设置许可位，提供访问控制）。
+ 将物理内存扩充成了更大的逻辑内存。
+ 通过将相应的虚拟页映射到同一物理页，可以实现共享库代码段。

### 页面置换算法有哪些？

1. 【无法实现】最佳置换法（OPT）

每次选择淘汰的页面是以后永不使用，或者在最长时间内不再被访问的页面。

2. 先进先出置换法（FIFO）

每次选择淘汰的页面是最早进入内存的页面。

3. 最近最久未使用置换法（LRU）

每次选择淘汰的页面是最近最久未使用的页面。

4. 时钟置换法（Clock）

是LRU算法的近似实现。维护位组成的循环队列，每个位代表一个页。当某页被访问时，其访问位置为1。当需要淘汰一个页面时，只需检查页的访问位。如果是0，就选择该页换出；如果是1，则将它置为0，暂不换出，继续检查下一个页面。

### 抖动指的是什么现象？

指页面不断地换进换出，导致CPU利用率低下。通常成因是经常使用的内存大小超出了物理内存大小。

### 碎片指的是什么现象？

+ 内部碎片：在一个已分配块比有效载荷大时发生的。也即分配了但不会被使用的空间。
+ 外部碎片：当空闲内存合计起来足够满足一个分配请求，但是没有一个单独的空闲块足够大可以来处理这个请求时发生的。

# 硬件结构

## 冯诺依曼模型由哪些部分组成？分别对应计算机的哪些硬件？

「运算器」、「控制器」、「存储器」、「输入设备」、「输出设备」。

+ 「运算器」对应CPU中的逻辑运算单元
+ 「控制器」对应CPU中的控制单元。

+ 「存储器」对应内存、磁盘、CPU中的寄存器、L1/L2/L3 Cache等存储设备。
+ 「输入设备」对应键盘、鼠标等。
+ 「输出设备」对应显示器等。

## 32位和64位CPU的最主要区别是什么？

一次能计算多少字节的数据。

+ 32位CPU一次可以计算4个字节。
+ 64位CPU一次可以计算8个字节。

## 为什么有了内存还需要寄存器？

内存离CPU的物理距离太远了，而寄存器在CPU中，且紧挨着CPU的控制单元和逻辑运算单元，运算速度更快。

### 常见的寄存器种类有哪些？

+ 「通用寄存器」，用来存放需要进行运算的数据。
+ 「程序计数器」，用来存储CPU将要执行的下一条指令「所在的内存地址」。
+ 「指令寄存器」，用来存放当前即将执行的指令，在指令执行完成之前，都存储在这里。

## 总线是用来干什么的？

总线用于CPU和内存以及其他设备之间的通信。

### 总线分为哪几种？

+ 「地址总线」，用于指定CPU将要操作的内存地址。
+ 「数据总线」，用于读写内存的数据。
+ 「控制总线」，用于发送和接收信号，比如中断、设备复位等信号。CPU收到信号后进行响应时，也需要控制总线。

### CPU是如何通过总线读写内存数据的？

1. 首先要通过「地址总线」来指定内存的地址。
2. 再通过「数据总线」来传输数据。

## CPU执行程序的过程？（一个指令周期，CPU做了什么？）

1. CPU读取「程序计数器」的值（即读取下一条指令的内存地址），然后CPU的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令传输给CPU，CPU收到数据后存入「指令寄存器」。
2. CPU分析「指令寄存器」中的指令，确定指令的类型和参数：如果是计算类型的指令，就将指令交由「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行。
3. CPU执行完指令后，「程序计数器」的值自增，执行下一条指令。自增的值的大小，由CPU的位宽决定，32位CPU的指令需要4个字节存放，因此自增4；而64位CPU则自增8。

## 64位CPU相比32位CPU的优势在哪？64位CPU的计算性能一定比32位CPU高吗？

64位CPU相比32位CPU的优势主要体现在两个方面：

1. 64位CPU可以一次计算32位~64位的数字，而32位CPU如果要计算超过32位的数字，就需要分多步骤进行计算，效率就没那么高。但是大部分应用程序很少会使用超过32位的数字。只有运算大数字的时候，64位CPU的优势才能体现出来，否则和32位CPU的计算性能相差不大。
2. 64位CPU可以寻址更大的内存空间。32位CPU最大的寻址地址是 4G=$2^{32}$，而64位CPU的最大寻址地址是 4E=$2^{64}$。

## 你知道软件的32位和64位之间的区别吗？32位的软件可以运行在64位机器上吗？反过来呢？

64位和32位的软件，实际上代表指令是64位还是32位的。

如果想要32位指令在64位机器上执行，只需要一套兼容机制，就可以做到了。

但如果想要64位指令在32位机器上执行，就比较困难了，因为32位的寄存器存不下64位的指令。

## 讲讲你对L1、L2、L3 Cache的理解？

L1 Cache 通常分为「数据缓存」和「指令缓存」，两者大小通常一致。

L1 Cache 和 L2 Cache 都是每个CPU核心独有的，而 L3 Cache 是多个CPU核心共享的。

CPU 访问 L1 Cache 需要2-4个时钟周期，访问 L2 Cache 需要10-20个时钟周期，访问 L3 Cache 需要20-60个时钟周期，访问寄存器一般少于一个时钟周期，访问内存需要200-300个时钟周期。

### 什么时候同步Cache和内存？

#### 写直达（Write Through）：把数据同时写入内存和 Cache 中。

+ 如果数据已经在 Cache 中，先更新 Cache，再更新内存。

+ 如果数据没有在 Cache 中，直接更新内存。

问题：无论数据在不在 Cache 里面，每次写操作都会操作内存，影响性能。

#### 写回（Write Back）：当发生写操作时，仅仅更新缓存，只有当缓存失效时，才更新内存。

+ 如果数据在缓存中，则更新缓存并标记。
+ 如果数据不在缓存中，并且对应缓存区域已被标记，则将缓存区域中的数据写回到内存中，再写入缓存。

如果我们大量操作都命中缓存，那么大部分时间里CPU都不需要读写内存，自然性能比写直达高很多。

### 如何保证缓存一致性？

1. 写传播（Write Propagation）：某个CPU核心里的Cache数据更新时，必须要传播到其他核心的Cache。
2. 事务的串行化（Transaction Serialization）：某个CPU核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的。

#### 总线嗅探

当CPU核进行写操作时，通过总线把这个事件广播给其他所有核心，每个CPU核心都会监听总线上的广播事件，并检查是否有相同的数据在自己的 L1/L2 Cache中，如果存在，则此时也更新 L1/L2 Cache。

问题：只保证了更新事件能被其他CPU核心知晓，并不能保证事务串行化。

#### MESI 协议

Modified（已修改）、Exclusive（独占）、Shared（共享）、Invalidated（已失效）。

使用上述的四个状态标记 Cache 中的数据。

Modifed：表示 Cache 中的数据已经修改，但还没有同步到内存中。

Exclusive：Cache 和内存是一致的，此时数据可以自由写入，而不用通知其他 CPU 核心。更新数据的同时标记为 Modified。

Shared：Cache 和内存是一致的，数据写入的时候，要先广播一个信号，要求其他核心中的对应 Cache 标记为 Invalidated，然后再更新当前 Cache 的数据，更新同时标记为 Modified。

### 谈谈你对伪共享的理解？

伪共享指的是多个 CPU 核心同时读写一个 Cache Line 的不同变量时，而导致的 CPU Cache 失效的现象。

#### 避免伪共享的方法

+ 在Linux内核中存在`__cacheline_aligned_in_smp`宏定义，采用这个宏定义可以使得变量在 Cache Line 中是对齐的。

## linux中的task调度算法

在 linux 中，根据任务的优先级以及响应要求，将 task 主要分为两种，其中优先级的数值越小，优先级越高。

+ 实时任务（优先级：0~99），对系统的响应时间要求很高，即要尽可能快地执行实时任务。
+ 普通任务（优先级：100~139），响应时间没有很高的要求。

linux 为了保障高优先级的任务能够尽可能早地被执行，分为3种调度类：

![调度类](https://raw.githubusercontent.com/ltlin9/note-img/master/img/2022/04/14/20220414-221737.png)

Deadline、Realtime这两个调度类，应用于实时任务，调度策略如下：

+ DEADLINE：安装 deadline 进行调度，距离当前时间点最近 deadline 的任务会被优先调度。
+ FIFO：对于相同优先级的任务，按照先来先服务原则，但是优先级更高的任务可以抢占低优先级的任务。
+ RR：对于相同优先级的任务，轮流运行一个时间片，但是优先级更高的任务可以抢占低优先级的任务。

Fair调度类应用于普通任务，有两种调度策略：

+ NORMAL：普通任务使用的调度策略。
+ BATCH：后台任务的调度策略，不会和终端交互，因此在不影响其他需要交互的任务的前提下，可以适当降低它的优先级。
+ CFS（完全公平调度）：为每个任务维护一个虚拟运行时间，优先选择虚拟运行时间少的任务。虚拟运行时间 = 时间运行时间 * NICE_0_LOAD / 权重。

### CPU 运行队列

每个CPU都有自己的运行队列，Deadline 运行队列 dl_rq，实时任务运行队列 rt_rq，CFS运行队列 cfs_rq，其中 cfs_rq 的数据结构为红黑树，最左结点即为下次被调度的任务。

CPU选择任务时，会按 Deadline > Realtime > Fair 的优先级顺序进行选择，因此实时任务总是会比普通任务优先被执行。

## 什么是软中断？

linux 为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「硬中断」和「软中断」。

+ 「硬中断」用来快速处理中断，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。「硬中断」会打断 CPU 正在执行的任务，然后立即执行中断处理程序。
+ 「软中断」用来延迟处理「硬中断」未完成的工作，一般以「内核线程」的形式运行。每一个CPU核心都对应一个软中断内核线程，名字通常为`ksoftirqd/「CPU编号」`。